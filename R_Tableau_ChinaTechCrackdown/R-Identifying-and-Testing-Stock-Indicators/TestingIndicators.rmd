---
title: "Identifying and Testing Stock Indicators"
author: "Austin McGhee"
date: "8/20/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include = FALSE, warning = FALSE, message = FALSE}

library(tidyverse)
library(tidyquant)
library(lubridate)
library(knitr)
library(ggrepel)
library(scales)
library(bdscale)
library(gridExtra)

```

```{r echo = FALSE, warning = FALSE, message = FALSE}
stock.list <- read_csv("stock_list.csv")
```


### Dollar Volume
In the price data we have dates and daily values for OHLC, adjusted and share volume.  We will create a new column called Dollar Volume (DV) which is the adjusted price multiplied by volume.  DV can be used to get an idea of money flow.  All prices are in US Dollar.

Using the DV we can measure both individual stocks and groups.  For example the DV for Didi may be $1 billion, and the sum total of DV for all US listed China Tech (>$3 billion Market Cap) may be $30 billion.  We could also calculate the average DV for the top five symbols and analyze it's relationship to the mean DV for all US listed China Tech.  We will refer to this filtered US listed China Tech as China Tech moving forward.
<p>&nbsp;</p>

```{r, echo=FALSE, warning=FALSE}

p1 <- stock.list %>%
  filter(date > "2020-01-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = volume * adjusted) %>%
  select(date, dollar_vol, Symbol) %>%
  group_by(date) %>%
  summarise(dollar_vol = sum(dollar_vol)/1000000000) %>%
  ggplot(aes(date, dollar_vol)) +
    geom_line() +
  geom_area(alpha=0.2) +
  labs(title = "Sum of Daily Dollar Volume for US Listed China Tech",
       subtitle = "Filter: Market Cap Over $3 Billion USD",
       y = "USD (Billions)", x="") +
  theme_tq() +
  scale_y_continuous(sec.axis = dup_axis())

p2 <- stock.list %>%
  filter(date > "2021-01-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = volume * adjusted) %>%
  select(date, dollar_vol, Symbol) %>%
  group_by(date) %>%
  summarise(dollar_vol = sum(dollar_vol)/1000000000) %>%
  ggplot(aes(date, dollar_vol)) +
    geom_line() +
  geom_area(alpha=0.2) +
  labs(title = "A Closer Look at 2021",
       y = "USD (Billions)", x="") +
  theme_tq() +
  scale_y_continuous(sec.axis = dup_axis())

grid.arrange(p1,p2, ncol = 1)

```
<p>&nbsp;</p>
We can see in the figure above that the sum of DV for China Tech is quite volatile but its general value tends to shift on a quarterly or annual basis.  From this data alone we can get an idea of the money flow in China Tech, but it doesn't provide insight into the direction of the underlying share price.


### Distribution
How is DV distributed among the individual stocks making up China Tech?  We could be looking at a basket of stocks with at similar DV, or a minority few making a disproportionate contribution.  In the chart below, each column represents a stock and the y axis (US Billions) is a measure of the average daily DV for the year.

```{r echo=FALSE, warning=FALSE, out.width="70%", message = FALSE}


P1 <- stock.list %>%
  filter(date > "2018-01-01") %>%
  filter(date < "2019-01-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = (volume * adjusted)/1000000000) %>%
  select(date, dollar_vol, Symbol) %>%
  group_by(Symbol) %>%
  summarise(doll_voll_avg = mean(dollar_vol)) %>%
  ggplot(aes(reorder(Symbol,doll_voll_avg), doll_voll_avg)) +
  geom_col() +
  labs(title = "2018",
       subtitle = "Highly concentrated in a single name",
       y = "USD (Billions)", x="Individual Stocks") +
  theme_tq() +
  scale_y_continuous(sec.axis = dup_axis()) +
  theme(
    legend.position = "NULL",
    axis.text.x=element_blank()) +
  ylim(0,4.5)

p2 <- stock.list %>%
  filter(date > "2019-01-01") %>%
  filter(date < "2020-01-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = (volume * adjusted)/1000000000) %>%
  select(date, dollar_vol, Symbol) %>%
  group_by(Symbol) %>%
  summarise(doll_voll_avg = mean(dollar_vol)) %>%
  ggplot(aes(reorder(Symbol,doll_voll_avg), doll_voll_avg)) +
  geom_col() +
  labs(title = "2019",
       subtitle = "",
       y = "USD (Billions)", x="Individual Stocks") +
  theme_tq() +
  scale_y_continuous(sec.axis = dup_axis()) +
  theme(
    legend.position = "NULL",
    axis.text.x=element_blank()) +
  ylim(0,4.5)

p3 <- stock.list %>%
  filter(date > "2020-01-01") %>%
  filter(date < "2021-01-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = (volume * adjusted)/1000000000) %>%
  select(date, dollar_vol, Symbol) %>%
  group_by(Symbol) %>%
  summarise(doll_voll_avg = mean(dollar_vol)) %>%
  ggplot(aes(reorder(Symbol,doll_voll_avg), doll_voll_avg)) +
  geom_col() +
  labs(title = "2020",
       subtitle = "",
       y = "USD (Billions)", x="Individual Stocks") +
  theme_tq() +
  scale_y_continuous(sec.axis = dup_axis()) +
  theme(
    legend.position = "NULL",
    axis.text.x=element_blank()) +
  ylim(0,4.5)

p4 <- stock.list %>%
  filter(date > "2021-01-01") %>%
  filter(date < "2021-09-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = (volume * adjusted)/1000000000) %>%
  select(date, dollar_vol, Symbol) %>%
  group_by(Symbol) %>%
  summarise(doll_voll_avg = mean(dollar_vol)) %>%
  ggplot(aes(reorder(Symbol,doll_voll_avg), doll_voll_avg)) +
  geom_col() +
  labs(title = "2021",
       subtitle = "Distribution of Daily Dollar Volume (Mean)",
       y = "USD (Billions)", x="Individual Stocks") +
  theme_tq() +
  scale_y_continuous(sec.axis = dup_axis()) +
  theme(
    legend.position = "NULL",
    axis.text.x=element_blank()) +
  ylim(0,4.5)

grid.arrange(p4,p3,p2,P1, ncol = 2)

```

We can see that DV is concentrated in 1-2 stocks each year and have progressively reduced concentration since 2018.  In the last four years, the first where there was a close runner-up to top rank was in 2021.  The next section identifies the individual stocks in these charts and explores them further.

<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

### Individual Stocks
Which stock is doing most of the heavy lifting? We can see that Alibaba (BABA) has been the DV leader every year.  The second spot has been Nio (NIO) or Baidu (BIDU).  These charts cover 2019-2021 but have replaced 2018 with a view of the last 3 months.  Based on this sample we can see that the composition of stocks in the 3 month view is the same as the annual view, suggesting composition remains the same in shorter time horizons.

```{r echo=FALSE, warning=FALSE, out.width="70%", message = FALSE}

p1 <- stock.list %>%
  filter(date > "2019-01-01") %>%
  filter(date < "2020-01-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = (volume * adjusted)/1000000000) %>%
  select(date, dollar_vol, Symbol) %>%
  group_by(Symbol) %>%
  summarise(doll_voll_avg = mean(dollar_vol)) %>%
  filter(doll_voll_avg > 0.25) %>%
  ggplot(aes(reorder(Symbol,doll_voll_avg), doll_voll_avg, fill = -doll_voll_avg)) +
  geom_col() +
  labs(title = "2019",
       y = "USD (Billions)", x="") +
  theme_tq() +
  ylim(0,5) +
  theme(
    legend.position = "NULL")

p2 <- stock.list %>%
  filter(date > "2020-01-01") %>%
  filter(date < "2021-01-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = (volume * adjusted)/1000000000) %>%
  select(date, dollar_vol, Symbol) %>%
  group_by(Symbol) %>%
  summarise(doll_voll_avg = mean(dollar_vol)) %>%
  filter(doll_voll_avg > 0.75) %>%
  ggplot(aes(reorder(Symbol,doll_voll_avg), doll_voll_avg, fill = -doll_voll_avg)) +
  geom_col() +
  labs(title = "2020",
       y = "USD (Billions)", x="") +
  theme_tq() +
  theme(
    legend.position = "NULL") +
    ylim(0,5)

p3 <- stock.list %>%
  filter(date > "2021-01-01") %>%
  filter(date < "2021-09-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = (volume * adjusted)/1000000000) %>%
  select(date, dollar_vol, Symbol) %>%
  group_by(Symbol) %>%
  summarise(doll_voll_avg = mean(dollar_vol)) %>%
  filter(doll_voll_avg > 0.75) %>%
  ggplot(aes(reorder(Symbol,doll_voll_avg), doll_voll_avg, fill = -doll_voll_avg)) +
  geom_col() +
  labs(title = "2021",
       y = "USD (Billions)", x="") +
  theme_tq() +
    ylim(0,5) +
  theme(
    legend.position = "NULL")

p4 <- stock.list %>%
  filter(date > "2021-06-01") %>%
  filter(date < "2021-09-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = (volume * adjusted)/1000000000) %>%
  select(date, dollar_vol, Symbol) %>%
  group_by(Symbol) %>%
  summarise(doll_voll_avg = mean(dollar_vol)) %>%
  filter(doll_voll_avg > 0.75) %>%
  ggplot(aes(reorder(Symbol,doll_voll_avg), doll_voll_avg, fill = -doll_voll_avg)) +
  geom_col() +
  labs(title = "Last 3 Months",
     y = "USD (Billions)", x="") +
  theme_tq() +
    ylim(0,5) +
  theme(
    legend.position = "NULL")

grid.arrange(p4,p3,p2,p1, ncol = 2)

```

The top stocks were selected by filtering by a minimum DV (Mean) of $500 million and a minimum market capitalization of $3 billion (August, 2021). In 2019 the minimum DV (Mean) filter was reduced to $250 million to show more results, otherwise it would only show Alibaba.
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

### Alibaba Comparisons
With Alibaba having the highest DV of China Tech we will focus our analysis on it.  How does Alibaba's performance compare to the rest of China Tech?  Can we find relative indicators in the DV by comparing Alibaba to the rest of China Tech?

First we will isolate Alibaba's DV and divide it by the China Tech DV (Sum).  This tells us the percent of China Tech (Sum) that Alibaba contributes on a daily basis.  We will focus on dates in the period January 2020 to August 2021.

```{r echo=FALSE, warning=FALSE, message = FALSE}
sl <- stock.list %>%
  filter(date > "2018-01-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = (volume * adjusted)/1000000000) %>%
  select(date, dollar_vol, Symbol) %>%
  group_by(date) %>%
  summarise(doll_voll_avg = sum(dollar_vol)) %>%
  rename(index.dv = doll_voll_avg)

sl2 <- stock.list %>%
  filter(Symbol %in% "BABA") %>%
  filter(date > "2018-01-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = (volume * adjusted)/1000000000) %>%
  select(date, dollar_vol, Symbol) %>%
  select(date, dollar_vol) %>%
  rename(baba.dv = dollar_vol)

slc <- merge(sl,sl2)

s1 <- slc %>%
  filter(date > "2020-01-01") %>%
  mutate(indexExBaba.LessBABA = (baba.dv/index.dv)*100) %>%
  select(date, indexExBaba.LessBABA) %>%
  ggplot(aes(date, indexExBaba.LessBABA)) +
  geom_line() +
  geom_smooth() +
  geom_area(alpha = 0.2) +
  labs(title = "Daily Dollar Value: (Sum) US Listed China Tech Less Alibaba",
       y = "Percent", x="") +
  theme_tq() +
  scale_y_continuous(sec.axis = dup_axis())

s2 <- slc %>%
  filter(date > "2021-03-01") %>%
  mutate(indexExBaba.LessBABA = (baba.dv/index.dv)*100) %>%
  select(date, indexExBaba.LessBABA) %>%
  ggplot(aes(date, indexExBaba.LessBABA)) +
  geom_line() +
  geom_area(alpha = 0.2) +
  labs(title = "A closer Look the last 6 months",
       y = "Percent", x="") +
  theme_tq() +
  scale_y_continuous(sec.axis = dup_axis())

grid.arrange(s1,s2, ncol = 1)

```
Looking at the data we can see that Alibaba is key contributor to the China Tech DV.  The relationship is volatile, but it's trendline ranges from 20-50%.  It's important to note that this an absolute measure, which is why I think it is important to reference shorter time periods.  Initial public offerings for instance will result in another stock that increases China Tech DV and will reduce the contribution of Alibaba to the total.

<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>


### Spread Comparison
Next we will look at the DV (Sum) spread between Alibaba and China Tech.  To calculate this we will first isolate Alibaba DV and deduct Alibaba from China Tech DV.  Now we are working with Alibaba DV and China Tech without Alibaba.  Next we take China Tech without Alibaba DV (Sum) and subtract Alibaba DV to create the spread.  

```{r echo=FALSE, warning=FALSE, out.width="90%",message = FALSE}

sl <- stock.list %>%
  filter(!Symbol %in% "BABA") %>%
  filter(date > "2018-01-01") %>%
  select(date, Symbol, volume, adjusted) %>%
  mutate(dollar_vol = (volume * adjusted)/1000000000) %>%
  select(date, dollar_vol, Symbol) %>%
  group_by(date) %>%
  summarise(doll_voll_avg = sum(dollar_vol)) %>%
  rename(index.dv = doll_voll_avg)

slc <- merge(sl,sl2)

c1 <- slc %>%
  filter(date > "2020-01-01") %>%
  mutate(Spread = index.dv-baba.dv) %>%
  select(date, Spread) %>%
  ggplot(aes(date, Spread)) +
  geom_line() +
  geom_hline(yintercept = 0, color = "red", size =1) +
  geom_area(alpha = 0.2) +
  labs(title = "DDV Spread",
       subtitle = "(Sum) Index Less Alibaba",
       y = "USD (Billions)", x="") +
  theme_tq()

c2 <- slc %>%
  filter(date > "2021-03-01") %>%
  mutate(Spread = index.dv-baba.dv) %>%
  select(date, Spread) %>%
  ggplot(aes(date, Spread)) +
  geom_line() +
  geom_hline(yintercept = 0, color = "red") +
  geom_area(alpha = 0.2) +
  labs(title = "A closer Look the last 6 months",
              subtitle = "Occurs infrequently",
       y = "USD (Billions)", x="") +
  theme_tq()

grid.arrange(c1,c2, ncol = 2)
```
We can see that the is volatile, but positive most of the time.  What does it mean when it's negative or positive?  Can be looked at as Alibaba having higher or lower DV compared to China Tech.  The values are absolute, we can reference the spread value with the y axis.

We can see that negative spreads are uncommon, especially since 2021.  However since 2021 they have become more erratic.  The chart below isolates the negative spreads to get a better visual representation.  The largest negative spread occurred on December 24, 2020 when the Ant Financial IPO was cancelled.  On that day Alibaba has over $20 billion more dollar volume than the rest of China Tech combined.

<p>&nbsp;</p>


```{r echo=FALSE, warning=FALSE, out.width="75%", message = FALSE}
slc %>%
  filter(date > "2020-01-01") %>%
  mutate(Spread = index.dv-baba.dv) %>%
  filter(Spread < 0) %>%
  select(date, Spread) %>%
  ggplot(aes(date, Spread)) +
  geom_hline(yintercept = 0, color = "black") +
  geom_jitter() +
  geom_col() +
  labs(title = "Observations of Negative Spreads Since January 2020",
       subtitle = "Value <0 indicates more transaction volume in Alibaba is higher than rest of US Listed China Shares",
       y = "USD (Billions)", x="",
       caption = "Minimum value on December 27, 2020 due to Ant IPO cancellation.") +
  theme_tq() +
  scale_y_continuous(sec.axis = dup_axis())

ddv <- slc

```

### Testing Negative DV Indicators
We can see that the number of occurrences where the spread is negative have decreased in 2021.  It may be worth exploring whether these occurrences can now serve as an indicator for next day price return.  If we take the occurrences (negative spread, date), match with the same day returns and then calculate next day returns we can test whether negative DV occurrences could be a useful indicator of returns in 2021.

Same day return refers tp the return of Alibaba on the date the negative spread occured.  Next day return refers to the next trading day.  It is worth noting that some same day returns with negative spreads will be positive and some will be negative.  Alibaba can have a negative spread of $2 billion and a positive same day return of 5%.  Returns are sorted by highest negative spread.

```{r, include=FALSE, warning=FALSE, message = FALSE}

# First we calculate the spread, then filter <0
store <- slc %>%
  filter(date > "2020-01-01") %>%
  mutate(Spread = index.dv-baba.dv) %>%
  select(date, Spread) %>%
  filter(Spread < 0)

# Taking stoking price data, using Tidyquant to get returns
baba.returns <- stock.list %>%
  filter(Symbol %in% "BABA") %>%
  filter(date > "2020-01-01") %>%
  select(date, adjusted) %>%
tq_transmute(adjusted,
             periodReturn,
             period = "daily",
             type = "log",
             col_rename = "returns")

# Next we combine date, spread, and adjusted together
df <- left_join(store, baba.returns, by = "date")

```

```{r, echo=FALSE, warning=FALSE, message = FALSE}

view <- df %>%
  arrange(-desc(Spread)) %>%
  mutate_if(is.numeric, round, 4) 
head(view)

```
This table has 66 total rows.  Next we will explore the relationship between the negative spread values and returns in more depth.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

p1 <- view %>%
  ggplot(aes(reorder(date, -Spread), -Spread)) +
    geom_col() +
  labs(title = "Negative Spread Distribution",
       y = "Spread", x="") +
    theme(axis.text.x=element_blank(),
        axis.title.x=element_blank()) 

p2 <- view %>%
  filter(Spread > -20) %>%
  ggplot(aes(reorder(date, -Spread), -Spread)) +
    geom_col() +
  labs(title = "Removing Ant IPO Outlier",
       y = "Spread", x="") +
    theme(axis.text.x=element_blank(),
        axis.title.x=element_blank()) 

p3 <- view %>%
  filter(Spread > -20) %>%
  filter(returns > 0) %>%
  ggplot(aes(reorder(date, -Spread), -Spread)) +
    geom_col() +
  labs(title = "Spreads with Negative Returns",
       y = "Spread", x="") +
    theme(axis.text.x=element_blank(),
        axis.title.x=element_blank()) +
  ylim(0,4)

p4 <- view %>%
  filter(Spread > -20) %>%
  filter(returns < 0) %>%
  ggplot(aes(reorder(date, Spread), -Spread)) +
    geom_col() +
  labs(title = "Spreads with Positive Returns",
       y = "Spread", x="") +
    theme(axis.text.x=element_blank(),
        axis.title.x=element_blank()) +
  ylim(0,4)

p5 <- view %>%
  filter(Spread > -20) %>%
  filter(returns < 0) %>%
  ggplot(aes(reorder(date, -returns), -returns*100)) +
  geom_col() +
  labs(title = "Distribution of Negative Returns",
       y = "Return", x="") +
  theme(axis.text.x=element_blank(),
        axis.title.x=element_blank()) +
  ylim(0,9)

p6 <- view %>%
  filter(Spread > -20) %>%
  filter(returns > 0) %>%
  ggplot(aes(reorder(date, -returns), returns*100)) +
  geom_col() +
  labs(title = "Distribution of Positive Returns",
       y = "% Return", x="") +
  theme(axis.text.x=element_blank(),
        axis.title.x=element_blank()) +
  ylim(0,9)

  
grid.arrange(p1,p2,p3,p4,p5,p6, ncol = 2)


```
Next we will explore the relationship between a negative spread and the same day return.  Although we have a small number of data points we can roughly show that higher absolute returns are associated with large (negative) DV spreads.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

p1 <- view %>%
  filter(Spread > -20) %>%
  filter(returns > 0) %>%
  ggplot(aes(-Spread, returns*100)) +
  geom_point() + 
  stat_smooth(method=lm) +  
  labs(title = "Positive Returns",
       y = "Same Day % Return", x="Negative Spread") +
  ylim(0,10) +
  xlim(0,4.5)

p2 <- view %>%
  filter(Spread > -20) %>%
  filter(returns < 0) %>%
  ggplot(aes(-Spread, -returns*100)) +
  geom_point() + 
  stat_smooth(method=lm) +  
  labs(title = "Negative Returns",
       y = "Same Day % Return", x="Negative Spread") +
  ylim(0,10) +
  xlim(0,4.5)


grid.arrange(p1,p2, ncol = 2)

```


Now we will filter for negative same day returns with negative spreads greater than 1.  We selected this negative filter value based on the the spreads for negative/positive return charts above.  It appears 1 is a level where there are fewer results and more volatility in returns.   The Ant IPO cancellation on December 24, 2020 is a major outlier.

We have 9 instances of negative returns with negative spreads greater than 1.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

# Splitting the positive and negative returns.
# Looking at negative first
df %>%
  filter(returns < 0) %>%
  filter(Spread < -1) %>%
  arrange(-desc(Spread)) %>%
  mutate_if(is.numeric, round, 4) %>%
print()

```

Now we will filter same day returns for positive returns.  We have 9 instances here as well.  We will remove the Ant IPO outlier for 8 instances.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

# Filtering by positive returns and sorting by largest spread
# Note
df %>%
  filter(returns > 0) %>%
  filter(Spread < -1) %>%
  arrange(-desc(Spread)) %>%
  mutate_if(is.numeric, round, 4) %>%
  print()

```
Outside of the major outlier in the negative 

Now we will take the dates from the previous dataframe and add a day to it.  Then we will take the dates and run them against the table of daily returns we have.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

# so now we will get the next days data.  we will add a day on to our data column and select that column only.  then do the same thing we did in the previous step.
dates <- df %>%
  filter(returns < 0) %>%
  filter(Spread < -1) %>%
  select(date)  %>%
  mutate(date = date + days (1))
dates
```
Matching the dates with returns, some are NA because the next business day comes on a weekend.  So we will isolate those and sim to the next day, and repeat that until we can get the next days return.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

# now take this added day and join like we did before match the dates with the returns,
comb <- left_join(dates,baba.returns, by = "date")

# Looks good, some dates NA, which are weekends.  We will sim ahead to the next day for those next and merge after
comb %>% mutate_if(is.numeric, round, 4) %>%
print()
```
These are the dates that we will keep and merge into a dateframe after we get the rest.  To get the rest we will push the dates forward another date and run against the returns table.  We will keep doing this until we get the next return for all dates.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

# get the NA and add a day to them
dates <- comb %>%
  filter(returns %in% NA) %>%
  select(date) %>%
  mutate(date = date + days (1))

# keep the ones we want for later
 comb <- comb %>%
  filter(!returns %in% NA)

# take a look at the output
comb %>% mutate_if(is.numeric, round, 4) %>%
  print()

```
Now we try for the next day.  There were no business days that fell on this day.  On to the next day then.  As we can see from the NA values.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

# now merge into dataframe to see the daily return for the next day
comb2 <- left_join(dates,baba.returns, by = "date")

# didnt work try again
print(comb2)

```
Looks like we got one more value by repeating the same process to the next one.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

# get the NA and add a day to them
dates <- comb2 %>%
  filter(returns %in% NA) %>%
  select(date) %>%
  mutate(date = date + days (1))

# join
comb3 <- left_join(dates,baba.returns, by = "date")

# take a look
comb3 %>% mutate_if(is.numeric, round, 4) %>%
print()

```

```{r, include=FALSE, warning=FALSE, message = FALSE}
# Add the day
dates <- comb3 %>%
  filter(returns %in% NA) %>%
  select(date) %>%
  mutate(date = date + days (1))

comb3 <- comb3 %>%
  filter(!returns %in% NA)

comb3 %>% mutate_if(is.numeric, round, 4) %>%
print()

```
Here are the last 2 dates we need. Omitting 2021-08-24 as that date was in the future when this report was generated.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

comb4 <- left_join(dates,baba.returns, by = "date")
# now we got the other 2.

comb4 %>% mutate_if(is.numeric, round, 4) %>%
print()

```


```{r, include=FALSE, warning=FALSE, message = FALSE}

# The Aug date is in the future, we will exclude it.
comb4 <- comb4 %>%
  filter(!returns %in% NA)

comb4 %>% mutate_if(is.numeric, round, 4) %>%
print()
```

Now we will join all the dates and returns into a single dataframe.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

merged <- full_join(comb, comb3)
merged <- full_join(merged, comb4)

merged %>% mutate_if(is.numeric, round, 4) %>%
print()

```


```{r, include=FALSE, warning=FALSE, message = FALSE}

temp <- merged %>%
  rename(next.day.return = returns) %>%
  arrange(desc(date)) %>%
  select(next.day.return)
temp %>% mutate_if(is.numeric, round, 4) %>%
print()


```


```{r, include=FALSE, warning=FALSE, message = FALSE}

t2 <- df %>%
  filter(returns < 0) %>%
  filter(Spread < -1) %>%
  arrange(-desc(Spread)) %>%
  filter(!date == "2021-08-20")
t2 %>% mutate_if(is.numeric, round, 4) %>%
print()

```
Now we put it all together in a dataframe.  We are looking at date, spread, same day, and next trading day return.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

performance <- data.frame(temp,t2) %>%
  select(date, Spread, same.day.return = returns, next.day.return)

performance %>% mutate_if(is.numeric, round, 4) %>%
print()
```
Now we will do the same for positive returns.  The table is below
```{r, include=FALSE, warning=FALSE, message = FALSE}

# Filtering by positive returns and sorting by largest spread
# Note
df %>%
  filter(returns > 0) %>%
  filter(Spread < -1) %>%
  arrange(-desc(Spread)) %>%
  mutate_if(is.numeric, round, 4) 

dates <- df %>%
  filter(returns > 0) %>%
  filter(Spread < -1) %>%
  select(date)  %>%
  mutate(date = date + days (1))

# now take this added day and join like we did before match the dates with the returns,
comb <- left_join(dates,baba.returns, by = "date")

# Looks good, some dates NA, which are weekends.  We will sim ahead to the next day for those next and merge after
comb %>% mutate_if(is.numeric, round, 4) %>%
  print()

# get the NA and add a day to them
dates <- comb %>%
  filter(returns %in% NA) %>%
  select(date) %>%
  mutate(date = date + days (1))

# keep the ones we want for later
comb <- comb %>%
  filter(!returns %in% NA)

# take a look at the output
comb %>% mutate_if(is.numeric, round, 4) %>%
  print()

# now merge into dataframe to see the daily return for the next day
comb2 <- left_join(dates,baba.returns, by = "date")

# didnt work try again
print(comb2)

# get the NA and add a day to them
dates <- comb2 %>%
  filter(returns %in% NA) %>%
  select(date) %>%
  mutate(date = date + days (1))

# join
comb3 <- left_join(dates,baba.returns, by = "date")

# take a look
comb3 %>% mutate_if(is.numeric, round, 4) %>%
  print()

merged <- full_join(comb, comb3)

merged %>% mutate_if(is.numeric, round, 4) %>%
  print()

temp <- merged %>%
  rename(next.day.return = returns) %>%
  arrange(desc(date)) %>%
  select(next.day.return)
temp %>% mutate_if(is.numeric, round, 4) %>%
  print()

t2 <- df %>%
  filter(returns > 0) %>%
  filter(Spread < -1) %>%
  arrange(-desc(Spread))
t2 %>% mutate_if(is.numeric, round, 4) %>%
  print()

performance2 <- data.frame(temp,t2) %>%
  select(date, Spread, same.day.return = returns, next.day.return)

```

```{r, echo=FALSE, warning=FALSE, message = FALSE}
performance2 %>% mutate_if(is.numeric, round, 4) %>%
  print()

```

Looking at the return charts again for the next day.

```{r, echo=FALSE, warning=FALSE, message = FALSE}

p1 <- performance2 %>%
  filter(Spread > -20) %>%
  ggplot(aes(-Spread, next.day.return*100)) +
  geom_point() +
  geom_hline(yintercept = 0.976, color = "red") +
  stat_smooth(method=lm) +  
  labs(title = "Next Day Return (Positive)",
       subtitle = "With Positive Same Day Return",
       y = "Next Day % Return", x="Negative Spread") +
  ylim(-4,7) +
  xlim(1,4)

p2 <- performance %>%
    filter(Spread > -20) %>%
  ggplot(aes(-Spread, next.day.return*100)) +
  geom_point() +
  geom_hline(yintercept = 0.264, color = "red") +
  stat_smooth(method=lm) +  
  labs(title = "Next Day Return (Negative)",
       subtitle = "With Negative Same Day Return",
       y = "Next Day % Return", x="Negative Spread") +
  ylim(-4,7) +
  xlim(1,4)

grid.arrange(p2,p1, ncol = 2)

```
Either return

```{r, echo=FALSE, warning=FALSE, message = FALSE}

all <- full_join(performance, performance2)

all <- all %>%
  mutate(next.day.return = next.day.return*100)

all %>%
  filter(Spread > -20) %>%
  ggplot(aes(-Spread,next.day.return)) +
    geom_point() +
    geom_hline(yintercept = 0.635, color = "red") +
    stat_smooth(method=lm) +  
    labs(
       title = "Combined Next Day Returns to Spread",
       subtitle = "With Both Positive and Negative Same Day Returns",
       y = "Next Day % Return", 
       x="Negative Spread") +
    ylim(-4,7) +
    xlim(1,4)
  


```

### Negative Same Day Return
All of the following values are in Percentage.
```{r, echo=FALSE, warning=FALSE, message = FALSE}

performance <- performance %>%
  filter(Spread > -20) %>%
  mutate(next.day.return = next.day.return*100)
performance %>%
  summarise(mean_next_day_return = mean(next.day.return)) %>%
  print()

```

Summary
```{r, echo=FALSE, warning=FALSE, message = FALSE}
summary(performance$next.day.return)
```
Maximum
```{r, echo=FALSE, warning=FALSE, message = FALSE}
max(performance$next.day.return)
```
Minimum
```{r, echo=FALSE, warning=FALSE, message = FALSE}
min(performance$next.day.return)
```

### Positive Same Day Return 
Percentage
```{r, echo=FALSE, warning=FALSE, message = FALSE}
performance2 <- performance2 %>%
  mutate(next.day.return = next.day.return*100)
performance2 %>%
  summarise(mean_next_day_return = mean(next.day.return)) %>%
  print()
```

Summary
```{r, echo=FALSE, warning=FALSE, message = FALSE}
summary(performance2$next.day.return)
```
Maximum
```{r, echo=FALSE, warning=FALSE, message = FALSE}
max(performance2$next.day.return)
```
Minimum
```{r, echo=FALSE, warning=FALSE, message = FALSE}
min(performance2$next.day.return)
```

### Combined (Positive/Negative) Same Day Return 
Percentage
```{r, echo=FALSE, warning=FALSE, message = FALSE}

all %>%
  summarise(mean_next_day_return = mean(next.day.return)) %>%
  print()
```

Summary
```{r, echo=FALSE, warning=FALSE, message = FALSE}
summary(all$next.day.return)
```
Maximum
```{r, echo=FALSE, warning=FALSE, message = FALSE}
max(all$next.day.return)
```
Minimum
```{r, echo=FALSE, warning=FALSE, message = FALSE}
min(all$next.day.return)
```

## Conclusion
Looking at the summary statistics we can see that for Alibaba on days where it's DV spread to China Tech (Sum) is over 1 billion and the same day return is positive, the average next day return is 0.97%.  With the same parameters, on days where the same day return is negative the average next day return is 0.26%.  The Ant IPO occurrence with a spread value of billion has been excluded in the summary statistics.

Based on this data, if one were to employ the strategy for positive return days they would do the following.  Wait for a negative spread occurrence greater than 1 to occur on a date that has a positive return.  To execute, this will require intraday volume and price data as close to the closing price as possible.  When an occurrence is observed, the purchase will be made as close to closing price as possible on that day.  Then the stock will be held until as close as possible the following day to be sold.

Thank you for taking the time to read my report.
